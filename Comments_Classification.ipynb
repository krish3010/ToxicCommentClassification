{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comments Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6RMbQHnmLyuD"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph-92goMHEKz",
        "colab_type": "text"
      },
      "source": [
        "# Importing Required Packages\n",
        "- nltk for Natural Language Processing\n",
        "- pandas for reading and writing csv files\n",
        "- numpy for manipulation of arrays\n",
        "- tensorflow for keras Deep Learning Module\n",
        "- keras for Deep Learning Modules\n",
        "- Sequential for building the deep neural network\n",
        "- LSTM for Long Short Term Memory for RNN (https://colah.github.io/posts/2015-08-Understanding-LSTMs/ found this very useful for learnning LSTMs)\n",
        "- Dense, Flatten, Embeddings for using them in the NNs\n",
        "- train_test_split to split our training data to training and validation sets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ITii9IkC0bt",
        "colab_type": "code",
        "outputId": "1038ed88-8818-43bd-8118-6a5acb545ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense, Flatten, Embedding, Activation\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haxB8JV2ImBi",
        "colab_type": "text"
      },
      "source": [
        "I am using my own google drive to upload my datasets. My university has a unlimited data that can be uploaded in my gdrive. It's easier for me to export the data at the desired location and save all my files in the same place. Keeps everything organized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiCJ0h-HC-83",
        "colab_type": "code",
        "outputId": "c5127cf9-d9a8-4e6b-bcfe-bd92cc9f1147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtAAR-JxJFSb",
        "colab_type": "text"
      },
      "source": [
        "Setting up a path variable so that I do not need to refer to it everytime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfQxEUMqDAFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path ='/content/drive/My Drive/Datasets/ToxicComment'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_peyA-u1JNHv",
        "colab_type": "text"
      },
      "source": [
        "# Data Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpFkuKXqDLDT",
        "colab_type": "code",
        "outputId": "665de304-f4e0-4103-96bf-8c83d49d57f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train = pd.read_csv(path + '/train.csv')\n",
        "test = pd.read_csv(path + '/test.csv')\n",
        "train.sample(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>77651</th>\n",
              "      <td>d003a549b62a8b9e</td>\n",
              "      <td>perhaps one of these notes to link to https://...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78179</th>\n",
              "      <td>d1432a20b6bb361f</td>\n",
              "      <td>January 2009 \\n Please stop your disruptive ed...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77045</th>\n",
              "      <td>ce57034a2003e303</td>\n",
              "      <td>These tags are completely bogus... the creatio...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23742</th>\n",
              "      <td>3eb5f46517faa982</td>\n",
              "      <td>\"\\n\\nOrphaned fair use image (Image:BTT UK 1.j...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158824</th>\n",
              "      <td>f4679504e5761d89</td>\n",
              "      <td>Check the map on the right, or here, or here...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      id  ... identity_hate\n",
              "77651   d003a549b62a8b9e  ...             0\n",
              "78179   d1432a20b6bb361f  ...             0\n",
              "77045   ce57034a2003e303  ...             0\n",
              "23742   3eb5f46517faa982  ...             0\n",
              "158824  f4679504e5761d89  ...             0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfvCoretDa9o",
        "colab_type": "code",
        "outputId": "ce42bb41-f44e-4895-a895-192cd6716eca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train.columns"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
              "       'insult', 'identity_hate'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZNSit3HJXG3",
        "colab_type": "text"
      },
      "source": [
        "Feature Engineering done from a Kaggle open Notebook (https://www.kaggle.com/eikedehling/feature-engineering). A huge thanks for Eike Dehling for such an awesome feature engineering algorithm.\n",
        "We need to do for both of the training and the testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txUK_lb4DcmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for dataset in [train,test]:\n",
        "  dataset['length'] = dataset['comment_text'].apply(len)\n",
        "  dataset['num_words'] = dataset['comment_text'].str.split().apply(len)\n",
        "  dataset['caps_letter'] = dataset['comment_text'].apply(lambda capsL: sum(1 for c in capsL if c.isupper()))\n",
        "  dataset['caps_word'] = dataset['comment_text'].apply(lambda capsW : sum(1 for c in capsW.split() if c.isupper()))\n",
        "  dataset['exclaimations'] = dataset['comment_text'].apply(lambda exclaim : exclaim.count('!'))\n",
        "  dataset['questions'] = dataset['comment_text'].apply(lambda questions : questions.count('?'))\n",
        "  dataset['punct'] = dataset['comment_text'].apply(lambda punct: sum(punct.count(p) for p in '.,;:'))\n",
        "  dataset['sp_char'] = dataset['comment_text'].apply(lambda sp_char: sum(sp_char.count(s) for s in '@#$%^*()-'))\n",
        "  dataset['unique_words'] = dataset['comment_text'].apply(lambda text: len(set(w for w in text.split())))\n",
        "  dataset['num_emoji'] = dataset['comment_text'].apply(lambda emoji: sum(emoji.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
        "\n",
        "  dataset['capsL_to_length'] = dataset['caps_letter'] / dataset['length']\n",
        "  dataset['capsW_to_length'] = dataset['caps_word'] / dataset['length']\n",
        "  dataset['unique_to_words'] = dataset['unique_words']/dataset['num_words']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H96Zg7ZjKR3A",
        "colab_type": "text"
      },
      "source": [
        "Taking the labels in the training set and putting in a separate variable for easier use in the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWA6TofSIpda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = pd.DataFrame(columns = ['toxic','severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n",
        "y_train['toxic'] = train ['toxic']\n",
        "y_train['severe_toxic'] = train['severe_toxic']\n",
        "y_train['obscene'] = train['obscene']\n",
        "y_train['threat'] = train['threat']\n",
        "y_train['insult'] = train['insult']\n",
        "y_train['identity_hate'] = train['identity_hate']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziJ81bLVKfDt",
        "colab_type": "text"
      },
      "source": [
        "Here I am going to set the max words of a single comment to be 500. From the first 500 words we could possibily detect what type of comment its going to be. This is also done to save computations.\n",
        "Max words in the comments is set to be 1000000. This is to convert all the words to one-hot vectors. \n",
        "https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f\n",
        "- Gives us what one hot encoder is.\n",
        "\n",
        "https://medium.com/@pemagrg/one-hot-encoding-129ccc293cda\n",
        "- Gives an idea on why we should do it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0nzQPgsISXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 500\n",
        "max_words = 1000000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMgsP30cLfDK",
        "colab_type": "text"
      },
      "source": [
        "We take all of the words in a single sentence so that we could apply the tokenizer and process them to make it as a one-hot vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4wSutwIEcGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_comments = train['comment_text']\n",
        "train_sentences = []\n",
        "test_comments = test['comment_text']\n",
        "test_sentences = []\n",
        "for i in range(len(train_comments)):\n",
        "    train_sentences.append(train_comments[i])\n",
        "for i in range(len(test_comments)):\n",
        "    test_sentences.append(test_comments[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62zY5i8CD_Cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "train_padded = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "tokenizer_test = Tokenizer(num_words=max_words)\n",
        "tokenizer_test.fit_on_texts(test_sentences)\n",
        "sequences_test = tokenizer_test.texts_to_sequences(test_sentences)\n",
        "word_index_test = tokenizer_test.word_index\n",
        "test_sequences = pad_sequences(sequences_test, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUzGPbjeLscO",
        "colab_type": "text"
      },
      "source": [
        "# Splitting the Training and Validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feJx7dD-E_EO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = y_train\n",
        "x_train, x_validate, y_train, y_validate = train_test_split(train_padded, labels, test_size=0.05, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RMbQHnmLyuD",
        "colab_type": "text"
      },
      "source": [
        "# Building the LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3FGZz7nuvT-",
        "colab_type": "text"
      },
      "source": [
        "I will be running the model only once. For my NN model, and the train test split I have done it took about 2 hours to complete 10 epochs reaching an accuracy of 99.05% on the training set and 98.76% on the validation set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHAJsXTdFDUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 32))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "fit_lstm = model.fit(x_train, y_train, epochs=10,batch_size=128, validation_data= (x_validate, y_validate))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReW9mi0QvThy",
        "colab_type": "text"
      },
      "source": [
        "We are going to save the model immediately after running 10 epochs. To know more about loading and saving models, the following link will be a good source. \n",
        "https://machinelearningmastery.com/save-load-keras-deep-learning-models/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SC2lOVsvV2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(path + '/commentsclassification.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed1TRRy6woMl",
        "colab_type": "text"
      },
      "source": [
        "# Loaded weights Initialization and beyond"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7esF4oYsvpCR",
        "colab_type": "text"
      },
      "source": [
        "This code is run after getting out our loaded weights for the classification. Since we need a fresh start, I restarted the kernel and ran code excluding 'Building the LSTM Model'\n",
        "\n",
        "This is the simplest explaination of transfer learning. I ran a model for 2 hours on a particular set of data. Using the weights I got at the last step I am making my predictions.\n",
        "\n",
        "I have a very strong feeling that transfer learning is the future of building an artificial intelligence.\n",
        "\n",
        "https://machinelearningmastery.com/transfer-learning-for-deep-learning/\n",
        "is a good start to learn more about transfer learning. \n",
        "\n",
        "If we had to do image classification task, it would be a good idea to take loaded weights from Inception-network. (https://keras.io/applications/#inceptionv3)\n",
        "\n",
        "Since we have to do word based classification, we need to build our own network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMGBxX14uc5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "2208a48f-9b8d-4e94-9f58-f52e8fd02d51"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model(path + '/commentsclassification.h5')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pDCvaE9HsgN",
        "colab_type": "code",
        "outputId": "b438a442-3b5b-4338-955f-7bb00ce03e7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 32)          32000000  \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 32,008,518\n",
            "Trainable params: 32,008,518\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptLwZaD4pD8W",
        "colab_type": "text"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1thxN2UbztM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "predict = model.predict(test_sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbgnNx36Ysyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA76k7BHbLWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample.to_csv(path + '/submission.csv',index= False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xt5REZW0Pgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}